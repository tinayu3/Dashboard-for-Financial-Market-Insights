{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ROUGE Scores ===\n",
      "\n",
      "rouge1:\n",
      "Precision: 0.3197\n",
      "Recall: 0.4875\n",
      "F1-Score: 0.3861\n",
      "\n",
      "rouge2:\n",
      "Precision: 0.0496\n",
      "Recall: 0.0759\n",
      "F1-Score: 0.0600\n",
      "\n",
      "rougeL:\n",
      "Precision: 0.1475\n",
      "Recall: 0.2250\n",
      "F1-Score: 0.1782\n",
      "\n",
      "Average F1 Score: 0.2081\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from typing import Dict, List\n",
    "\n",
    "class AnswerEvaluator:\n",
    "    def __init__(self):\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    def calculate_rouge_scores(self, generated_text: str, reference_text: str) -> Dict:\n",
    "        \"\"\"ROUGE Score\"\"\"\n",
    "        scores = self.scorer.score(reference_text, generated_text)\n",
    "        \n",
    "        return {\n",
    "            'rouge1': {\n",
    "                'precision': scores['rouge1'].precision,\n",
    "                'recall': scores['rouge1'].recall,\n",
    "                'fmeasure': scores['rouge1'].fmeasure\n",
    "            },\n",
    "            'rouge2': {\n",
    "                'precision': scores['rouge2'].precision,\n",
    "                'recall': scores['rouge2'].recall,\n",
    "                'fmeasure': scores['rouge2'].fmeasure\n",
    "            },\n",
    "            'rougeL': {\n",
    "                'precision': scores['rougeL'].precision,\n",
    "                'recall': scores['rougeL'].recall,\n",
    "                'fmeasure': scores['rougeL'].fmeasure\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def evaluate_answer(self, generated_text: str, reference_text: str) -> Dict:\n",
    "        \"\"\"Overal results\"\"\"\n",
    "        rouge_scores = self.calculate_rouge_scores(generated_text, reference_text)\n",
    "        \n",
    "        return {\n",
    "            'rouge_scores': rouge_scores,\n",
    "            'average_f1': sum(score['fmeasure'] for score in rouge_scores.values()) / 3\n",
    "        }\n",
    "\n",
    "# Execution\n",
    "def main():\n",
    "    # test text\n",
    "    generated = \"\"\"Claiming business expenses for a business with no income can be a tricky task, but it is not impossible. \n",
    "    There are a few things to keep in mind when claiming business expenses when you have no income. First, you will need to be able \n",
    "    to prove that the expenses were incurred in the course of running your business. This can be done by providing receipts, \n",
    "    invoices, or other documentation. You will also need to keep track of your expenses throughout the year so that you can \n",
    "    accurately report them on your tax return. Finally, it is important to consult with a tax professional to ensure that you \n",
    "    are taking advantage of all of the deductions and credits that are available to you.\"\"\"\n",
    "\n",
    "    reference = \"\"\"Yes you can claim your business deductions if you are not making any income yet. But first you should decide \n",
    "    what structure you want to have for your business. Either a Company structure or a Sole Trader or Partnership. If you choose \n",
    "    a Company Structure (which is more expensive to set up) you would claim your deductions but no income. So you would be making \n",
    "    a loss, and continue making losses until your income from the business exceed your expenses.\"\"\"\n",
    "\n",
    "    # Evalution\n",
    "    evaluator = AnswerEvaluator()\n",
    "    results = evaluator.evaluate_answer(generated, reference)\n",
    "\n",
    "    # Result\n",
    "    print(\"\\n=== ROUGE Scores ===\")\n",
    "    for rouge_type, scores in results['rouge_scores'].items():\n",
    "        print(f\"\\n{rouge_type}:\")\n",
    "        print(f\"Precision: {scores['precision']:.4f}\")\n",
    "        print(f\"Recall: {scores['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {scores['fmeasure']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nAverage F1 Score: {results['average_f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1981c4e36efd4bcba1e01ec7b68e415f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cfa7d956a14e2a916c38e7d37813d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bc7970c622478487912d8c93250df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744fa7d322ed43608c0f23d1e7a25cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c242b8a55836453b8fcc2ef7f2a43c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddb802314674ab9bcaad554243b4247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7214d0ce972942508518659dd4a9ac01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54f2fd72afd4434bfeefd0530959742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062e6027ed654a7392a710ef3b2f024b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa89ca51a0db4d2387418ed2ea93f8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e7c36ae8fb46779b886592262f5668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semantic Similarity Results ===\n",
      "Overall Similarity: 0.6428\n",
      "Maximum Similarity: 0.7262\n",
      "Minimum Similarity: 0.4854\n",
      "\n",
      "=== Key Concept Coverage ===\n",
      "business expenses claiming: 0.6531\n",
      "no income situation: 0.4554\n",
      "business structure: 0.1644\n",
      "tax deductions: 0.2924\n",
      "company formation: 0.0897\n",
      "\n",
      "Average Concept Coverage: 0.3310\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, List\n",
    "\n",
    "class SemanticEvaluator:\n",
    "    def __init__(self):\n",
    "        # Load model for sentence embedding\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def calculate_semantic_similarity(self, generated_text: str, reference_text: str) -> Dict:\n",
    "        \"\"\"Sentence-level semantic similarity calculation\"\"\"\n",
    "        # Split sentence\n",
    "        gen_sentences = generated_text.split('. ')\n",
    "        ref_sentences = reference_text.split('. ')\n",
    "        \n",
    "        # Embedding\n",
    "        gen_embeddings = self.model.encode(gen_sentences)\n",
    "        ref_embeddings = self.model.encode(ref_sentences)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity_matrix = cosine_similarity(gen_embeddings, ref_embeddings)\n",
    "        \n",
    "        # Maximum similarity at each sentence\n",
    "        max_similarities = np.max(similarity_matrix, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'overall_similarity': float(np.mean(max_similarities)),\n",
    "            'sentence_similarities': [float(sim) for sim in max_similarities],\n",
    "            'max_similarity': float(np.max(max_similarities)),\n",
    "            'min_similarity': float(np.min(max_similarities))\n",
    "        }\n",
    "    \n",
    "    def analyze_key_concepts(self, text: str, key_concepts: List[str]) -> Dict:\n",
    "        \"\"\"Analysis of semantic inclusion of key concepts\"\"\"\n",
    "        text_embedding = self.model.encode([text])[0]\n",
    "        concept_embeddings = self.model.encode(key_concepts)\n",
    "        \n",
    "        similarities = cosine_similarity([text_embedding], concept_embeddings)[0]\n",
    "        \n",
    "        concept_coverage = {}\n",
    "        for concept, similarity in zip(key_concepts, similarities):\n",
    "            concept_coverage[concept] = float(similarity)\n",
    "            \n",
    "        return {\n",
    "            'concept_coverage': concept_coverage,\n",
    "            'average_coverage': float(np.mean(similarities))\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    generated = \"\"\"Claiming business expenses for a business with no income can be a tricky task, but it is not impossible. \n",
    "    There are a few things to keep in mind when claiming business expenses when you have no income. First, you will need to be able \n",
    "    to prove that the expenses were incurred in the course of running your business.\"\"\"\n",
    "\n",
    "    reference = \"\"\"Yes you can claim your business deductions if you are not making any income yet. But first you should decide \n",
    "    what structure you want to have for your business. Either a Company structure or a Sole Trader or Partnership.\"\"\"\n",
    "    \n",
    "    key_concepts = [\n",
    "        \"business expenses claiming\",\n",
    "        \"no income situation\",\n",
    "        \"business structure\",\n",
    "        \"tax deductions\",\n",
    "        \"company formation\"\n",
    "    ]\n",
    "\n",
    "    evaluator = SemanticEvaluator()\n",
    "    \n",
    "    # Semantic similarity assessment\n",
    "    similarity_results = evaluator.calculate_semantic_similarity(generated, reference)\n",
    "    \n",
    "    # Analysis including key concepts\n",
    "    concept_results = evaluator.analyze_key_concepts(generated, key_concepts)\n",
    "    \n",
    "    # Result\n",
    "    print(\"\\n=== Semantic Similarity Results ===\")\n",
    "    print(f\"Overall Similarity: {similarity_results['overall_similarity']:.4f}\")\n",
    "    print(f\"Maximum Similarity: {similarity_results['max_similarity']:.4f}\")\n",
    "    print(f\"Minimum Similarity: {similarity_results['min_similarity']:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Key Concept Coverage ===\")\n",
    "    for concept, score in concept_results['concept_coverage'].items():\n",
    "        print(f\"{concept}: {score:.4f}\")\n",
    "    print(f\"\\nAverage Concept Coverage: {concept_results['average_coverage']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
